{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10ecd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a710e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e6d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet(df: pd.DataFrame):\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            print(f\"Created temporary directory: {temp_dir}\")\n",
    "            temp_file_path = os.path.join(temp_dir, 'intermediate_data.parquet')\n",
    "\n",
    "            # Save intermediate results\n",
    "            df.to_parquet(temp_file_path)\n",
    "            print(f\"Saved intermediate data to {temp_file_path}\")\n",
    "\n",
    "            # Load and perform further processing...\n",
    "            loaded_data = pd.read_parquet(temp_file_path)\n",
    "            print(\"Loaded intermediate data for next step.\")\n",
    "            return loaded_data\n",
    "\n",
    "        # Outside the 'with' block:\n",
    "        print(f\"Temporary directory {temp_dir} exists? {os.path.exists(temp_dir)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c1141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/AI_Human.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e8d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmp0pc9gm1o\n",
      "Saved intermediate data to /tmp/tmp0pc9gm1o/intermediate_data.parquet\n",
      "Loaded intermediate data for next step.\n"
     ]
    }
   ],
   "source": [
    "df = load_parquet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d91b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0.0    305797\n",
       "1.0    181438\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300fa6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0.0\n",
       "generated    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054ce0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. Configure the model you want to use\n",
    "MODEL_CONFIG = {\n",
    "    \"id\": \"meta-llama/Llama-3.2-1B\",\n",
    "    \"name\": \"Llama 3.2 1B\",\n",
    "    \"torch_dtype\": torch.float16\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"Loads the specified model and tokenizer from Hugging Face.\"\"\"\n",
    "    model_id = config[\"id\"]\n",
    "    try:\n",
    "        # Load tokenizer and add a padding token if it's missing\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Load the model with device_map for automatic hardware placement (GPU/CPU)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=config[\"torch_dtype\"],\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {config['name']}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"Calculates the perplexity of a given text using a specified model.\"\"\"\n",
    "    if not text.strip() or not model or not tokenizer:\n",
    "        return float('inf')\n",
    "\n",
    "    try:\n",
    "        # Move model to the correct device\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        \n",
    "        # Calculate the loss, which is the negative log-likelihood\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            \n",
    "        # Exponentiate the loss to get perplexity\n",
    "        perplexity = torch.exp(neg_log_likelihood)\n",
    "        torch.cuda.empty_cache()\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in perplexity calculation: {e}\")\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b9fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... This might take a moment.\n",
      "Model loaded successfully. Calculating perplexity...\n",
      "\n",
      "Human Text Perplexity: 7.05\n",
      "AI Text Perplexity (Thesaurus version): 417.04\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "print(\"Loading model... This might take a moment.\")\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_CONFIG)\n",
    "\n",
    "if model and tokenizer:\n",
    "    print(\"Model loaded successfully. Calculating perplexity...\")\n",
    "    # A standard, common sentence\n",
    "    human_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    # A more verbose, thesaurus-like version, typical of some AI outputs\n",
    "    ai_text = \"The rapid, cinnamon-colored vulpine creature elegantly leaps across the indolent canine.\"\n",
    "\n",
    "    human_perplexity = calculate_perplexity(human_text, model, tokenizer)\n",
    "    ai_perplexity = calculate_perplexity(ai_text, model, tokenizer)\n",
    "\n",
    "    print(f\"\\nHuman Text Perplexity: {human_perplexity:.2f}\")\n",
    "    print(f\"AI Text Perplexity (Thesaurus version): {ai_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d3d8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "def save_checkpoint(filename, pyobject):\n",
    "    with open(f'../data/checkpoint/{filename}.pkl', mode='wb') as pklf:\n",
    "        pickle.dump(pyobject, pklf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_last_checkpoint(checkpoint_dir):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    file_idx = []\n",
    "    for fp in glob.glob(f'{checkpoint_dir}/*.pkl'):\n",
    "        fn = os.path.basename(fp)\n",
    "        file_idx.append(int(re.findall(r'\\d+', fn)[0]))\n",
    "    last_checkpoint_idx = max(file_idx)\n",
    "    with open(f'../data/checkpoint/perplexity_{last_checkpoint_idx}.pkl', mode='rb') as pklf:\n",
    "        last_checkpoint_list = pickle.load(pklf)\n",
    "    return last_checkpoint_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3cd5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.realpath('../data/checkpoint')\n",
    "if len(glob.glob(f'{checkpoint_dir}/*.pkl')) == 0:\n",
    "    perplexity_list = []\n",
    "else:\n",
    "    perplexity_list = load_last_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49fb1923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(perplexity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10002/487235 [00:00<00:04, 98940.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n",
      "10002\n",
      "10003\n",
      "10004\n",
      "10005\n",
      "10006\n",
      "10007\n",
      "10008\n",
      "10009\n",
      "10010\n",
      "10011\n",
      "10012\n",
      "10013\n",
      "10014\n",
      "10015\n",
      "10016\n",
      "10017\n",
      "10018\n",
      "10019\n",
      "10020\n",
      "10021\n",
      "10022\n",
      "10023\n",
      "10024\n",
      "10025\n",
      "10026\n",
      "10027\n",
      "10028\n",
      "10029\n",
      "10030\n",
      "10031\n",
      "10032\n",
      "10033\n",
      "10034\n",
      "10035\n",
      "10036\n",
      "10037\n",
      "10038\n",
      "10039\n",
      "10040\n",
      "10041\n",
      "10042\n",
      "10043\n",
      "10044\n",
      "10045\n",
      "10046\n",
      "10047\n",
      "10048\n",
      "10049\n",
      "10050\n",
      "10051\n",
      "10052\n",
      "10053\n",
      "10054\n",
      "10055\n",
      "10056\n",
      "10057\n",
      "10058\n",
      "10059\n",
      "10060\n",
      "10061\n",
      "10062\n",
      "10063\n",
      "10064\n",
      "10065\n",
      "10066\n",
      "10067\n",
      "10068\n",
      "10069\n",
      "10070\n",
      "10071\n",
      "10072\n",
      "10073\n",
      "10074\n",
      "10075\n",
      "10076\n",
      "10077\n",
      "10078\n",
      "10079\n",
      "10080\n",
      "10081\n",
      "10082\n",
      "10083\n",
      "10084\n",
      "10085\n",
      "10086\n",
      "10087\n",
      "10088\n",
      "10089\n",
      "10090\n",
      "10091\n",
      "10092\n",
      "10093\n",
      "10094\n",
      "10095\n",
      "10096\n",
      "10097\n",
      "10098\n",
      "10099\n",
      "10100\n",
      "10101\n",
      "10102\n",
      "10103\n",
      "10104\n",
      "10105\n",
      "10106\n",
      "10107\n",
      "10108\n",
      "10109\n",
      "10110\n",
      "10111\n",
      "10112\n",
      "10113\n",
      "10114\n",
      "10115\n",
      "10116\n",
      "10117\n",
      "10118\n",
      "10119\n",
      "10120\n",
      "10121\n",
      "10122\n",
      "10123\n",
      "10124\n",
      "10125\n",
      "10126\n",
      "10127\n",
      "10128\n",
      "10129\n",
      "10130\n",
      "10131\n",
      "10132\n",
      "10133\n",
      "10134\n",
      "10135\n",
      "10136\n",
      "10137\n",
      "10138\n",
      "10139\n",
      "10140\n",
      "10141\n",
      "10142\n",
      "10143\n",
      "10144\n",
      "10145\n",
      "10146\n",
      "10147\n",
      "10148\n",
      "10149\n",
      "10150\n",
      "10151\n",
      "10152\n",
      "10153\n",
      "10154\n",
      "10155\n",
      "10156\n",
      "10157\n",
      "10158\n",
      "10159\n",
      "10160\n",
      "10161\n",
      "10162\n",
      "10163\n",
      "10164\n",
      "10165\n",
      "10166\n",
      "10167\n",
      "10168\n",
      "10169\n",
      "10170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10171/487235 [00:11<09:10, 866.63it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(perplexity_list):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m perplexity_list\u001b[38;5;241m.\u001b[39mappend(perplexity)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx)\n",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[0;34m(text, model, tokenizer)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Exponentiate the loss to get perplexity\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(neg_log_likelihood)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Github/ai-text-detection/notebooks/.venv/lib/python3.10/site-packages/torch/cuda/memory.py:224\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 224\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.realpath('../data/checkpoint')\n",
    "if len(glob.glob(f'{checkpoint_dir}/*.pkl')) == 0:\n",
    "    perplexity_list = []\n",
    "else:\n",
    "    perplexity_list = load_last_checkpoint(checkpoint_dir)\n",
    "\n",
    "for idx, txt in enumerate(tqdm(df['text'])):\n",
    "    if idx <= len(perplexity_list):\n",
    "        continue\n",
    "    \n",
    "    perplexity = calculate_perplexity(txt, model, tokenizer)\n",
    "    perplexity_list.append(perplexity)\n",
    "    \n",
    "    if idx %  10000 == 0:\n",
    "        print(f'Creating checkpoint_{idx}')\n",
    "        filename = f'perplexity_{idx}'\n",
    "        save_checkpoint(filename, perplexity_list)\n",
    "        print(f\"{filename} created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c3b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487230</th>\n",
       "      <td>Tie Face on Mars is really just a big misunder...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487231</th>\n",
       "      <td>The whole purpose of democracy is to create a ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487232</th>\n",
       "      <td>I firmly believe that governments worldwide sh...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487233</th>\n",
       "      <td>I DFN't agree with this decision because a LFT...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487234</th>\n",
       "      <td>Richard Non, Jimmy Carter, and Bob Dole and ot...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>487235 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  generated\n",
       "0       Cars. Cars have been around since they became ...        0.0\n",
       "1       Transportation is a large necessity in most co...        0.0\n",
       "2       \"America's love affair with it's vehicles seem...        0.0\n",
       "3       How often do you ride in a car? Do you drive a...        0.0\n",
       "4       Cars are a wonderful thing. They are perhaps o...        0.0\n",
       "...                                                   ...        ...\n",
       "487230  Tie Face on Mars is really just a big misunder...        0.0\n",
       "487231  The whole purpose of democracy is to create a ...        0.0\n",
       "487232  I firmly believe that governments worldwide sh...        1.0\n",
       "487233  I DFN't agree with this decision because a LFT...        0.0\n",
       "487234  Richard Non, Jimmy Carter, and Bob Dole and ot...        0.0\n",
       "\n",
       "[487235 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
